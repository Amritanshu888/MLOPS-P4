{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7943e110",
   "metadata": {},
   "source": [
    "- nvidia-smi --> library needs to be installed\n",
    "- Model which we are using : google/pegasus-cnn_dailymail --> for my text summarization task and we will be using samsun data(Samsung data that we have) on huggingface.\n",
    "- This data has 3 important fields : 1. Dialogue: Text of dialogue 2. Summary: human written summary of the dialogue 3. id: unique id of an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e07cda",
   "metadata": {},
   "source": [
    "- Whenever we talk abt this google/pegasus-cnn_dailymail model --> This is based on a sequence to sequence model, so for sequence to sequence model u specifically require a tokenizer. When u click Use this model(in black available on right hand side) -> u will get the codes u need to execute this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729ba8f",
   "metadata": {},
   "source": [
    "- rogue_score will be the performance metrics to calculate for this particular model\n",
    "- Other libraries required: accelerate, transformers accelerate(These libraries are provided by huggingface itself).\n",
    "- We specifically use the above libraries to make sure that we assign all the jobs in a better way to the GPUs when our training specifically happens. That is the reason why we specifically use accelerate.\n",
    "\n",
    "## Purpose of accelerate:\n",
    "1. Ease of Multi-Device Training: Whether you're using multiple GPUs or TPUs, accelerate makes it easier to scale your model across devices with minimal code changes.\n",
    "2. Mixed Precision: It allows models to be trained using mixed precision, which can speed up training and reduce memory usage.\n",
    "3. Zero Redundancy Optimizer (ZeRO): Helps manage large models by efficiently splitting the model across multiple devices.\n",
    "4. Offload to CPU/SSD: Useful for large models that may not fit entirely into GPU memory, by allowing parts of the model or optimizer to be offloaded to CPU or even SSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f33da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "# from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer ## AutoTokenizer is a library which is specifically used to convert ur text into tokens(So for every model that is available in huggingface there will be an AutoTokenizer which will be compatible to that model and its responsibility is to basically convert ur text into token).\n",
    "## AutoModelForSeq2SeqLM ---> This is basically used just to load the particular model u are specifically using\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb0ee0",
   "metadata": {},
   "source": [
    "## Basic Functionality of Huggingface Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7329ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amritanshu Bhardwaj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Amritanshu Bhardwaj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"California's largest electricity provider has turned off power to hundreds of thousands of customers across the state.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\") ## Loading the model  1st we are loading the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")  ## Converting words into tokens    2nd we are loading the tokenizer\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousands customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tommorow.\"\n",
    ") ## This is my article\n",
    "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\") ## Applying tokenizer to article summary, we are returning the tensors in pt format\n",
    "## Above we are converting the articles into tokens\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])  ## Generating the summarizer for the text above, it will generate something called as \"input_ids\", generating based on the tokens\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]  ## Converting back from my ids to my text(Converting back from tokens to text)\n",
    "## skip_special_tokens=True will remove all the unnecessary tokens that are present inside like the space token or the clear token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92e47ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" ## If torch.cuda is available or not --> it will display whether we are going to use GPU or not\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942185f4",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b86ea",
   "metadata": {},
   "source": [
    "## This will give us the idea that how can we do finetuning on any kind of model and for any kind of dataset that we have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ce395",
   "metadata": {},
   "source": [
    "## Our main aim here is to finetune with the custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1767cc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Couldn't instantiate the backend tokenizer from one of: \n",
      "(1) a `tokenizers` library serialization file, \n",
      "(2) a slow tokenizer instance to convert or \n",
      "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
      "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import transformers\n",
    "\n",
    "\n",
    "# Load model with explicit configuration\n",
    "model_name = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "# Try loading with different parameters\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)  # Force slow tokenizer --> ## Main work is to convert the text into tokens\n",
    "    model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device) ## Here we are basically loading the model\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "## This code is used just to load the model and the tokenizer that is used for this particular model that is \"pegasus-cnn_dailymail\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08595bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 7.90M/7.90M [00:01<00:00, 6.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n",
      "Extracting files...\n",
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "## download & unzip data\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "# Download the file\n",
    "url = \"https://github.com/krishnaik06/datasets/raw/refs/heads/main/summarizer-data.zip\"\n",
    "print(\"Downloading dataset...\")\n",
    "\n",
    "# Download with progress bar\n",
    "response = requests.get(url, stream=True)\n",
    "total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "with open(\"summarizer-data.zip\", \"wb\") as file:\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            file.write(data)\n",
    "            pbar.update(len(data))\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "# Extract the zip file\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(\"summarizer-data.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    \n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc85a64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum = load_from_disk('samsum_dataset')  ## If u use this load_from_disk function it will load that data and convert it into a dictionary\n",
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0757a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum] ## Here in this code we are trying to understand how much is the length of train data, test data and validation data.\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "## There are 3 features : dialogue, id and summary in both train and test\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "print(\"\\nSummary\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5930763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Lenny:  <file_photo>\n",
      "Bob: I like the first ones best\n",
      "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
      "Bob: I have four black pairs :D :D\n",
      "Lenny: yeah, but shouldn't I pick a different color?\n",
      "Bob: what matters is what you'll give you the most outfit options\n",
      "Lenny: So I guess I'll buy the first or the third pair then\n",
      "Bob: Pick the best quality then\n",
      "Lenny: ur right, thx\n",
      "Bob: no prob :)\n",
      "\n",
      "Summary\n",
      "Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n"
     ]
    }
   ],
   "source": [
    "## See the same for next test data\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum] ## Here in this code we are trying to understand how much is the length of train data, test data and validation data.\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "## There are 3 features : dialogue, id and summary in both train and test\n",
    "print(dataset_samsum[\"test\"][2][\"dialogue\"]) ## For next test data\n",
    "\n",
    "print(\"\\nSummary\")\n",
    "\n",
    "print(dataset_samsum[\"test\"][2][\"summary\"]) # For next test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd834c",
   "metadata": {},
   "source": [
    "## Preparing Data For Training For Sequence To Sequence Model\n",
    "- {\n",
    "    'dialogue': \"Hi! How are you?\",\n",
    "    'summary': \"The speaker is asking how the other person is.\"\n",
    "}\n",
    "- This above data needs to be converted into 3 main fields which are given below.\n",
    "- Here u have input_ids(which is token ids for the dialogue), whatever input tokenizer we are using it will convert ur words into tokens. -> This is required for the training of any sequence to sequence model.\n",
    "- Then we have attention mask which is used to apply some special characters within this word in the form of tokens -> which will be useful for the sequence to sequence model to implement or to do the prediction and finetune the model.\n",
    "- Third parameter that we have is labels : It is basically Token ID for the summar('target' feature). For 'summary'(target feature) we will apply the tokenizer and convert these words into this particular tokens.\n",
    "- This has to be done before i pass my data to Sequence to Sequence Model.\n",
    "\n",
    "- {\n",
    "    'input_ids': [123,456,789, ...], # Token IDs for the dialogue\n",
    "    'attention_mask': [1,1,1, ...], # Attention mask for the input\n",
    "    'labels': [321,654,987, ...], # Token IDs for the summary (target)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c32646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length = 1024, truncation = True) ## To convert into input encodings i will be using a tokenizer\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True) ## Also converting our summary into tokens, but here we have to use this tokenizer as a target tokenizer for that we use the function as_target_tokenizer()\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    } ## Here we are basically returning our 'input_ids', 'attention_mask' and 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1eaee18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf1eab2216b41b19145977c04e856ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amritanshu Bhardwaj\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca69c9a3c0054ab8a4eb0cebd7062b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d16a59f5fb4320a49524ce15ff90af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Now the above particular function i can apply to my entire dataset of samsum which is in the form of dictionary\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede1c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['train'] ## Now here u can see that in features i have more additional fields : 'input_ids', 'attention_mask', 'labels'\n",
    "## These have also been added to my dataset. -> The new features are really important for training purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca340249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724be7c3",
   "metadata": {},
   "source": [
    "- DataCollatorForSeq2Seq is a special data collator designed for sequence-to-sequence models(e.g., Pegasus,T5,BART) that helps in preparing batches of data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b22cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq  ## When we are training a sequence to sequence model we need to use this DataCollator\n",
    "## What it does is that : This make sures that whatever data we specifically have it tries to convert that into batch so that it can be provided to the model for the training purpose\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b7c47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "## In order to finetune our data: Initially we prepared it by adding additional features above\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir = 'pegasus-samsum', num_train_epochs=1, warmup_steps=500,  ## Reason why number of epochs we have kept 1 is that it is a very huge dataset\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"test\"], ## Instead of givng train_dataset i m giving test_dataset as its smaller that the train dataset\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"]) ## Evaluation Dataset i will set to validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1089ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b07f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation\n",
    "### 1st[1,2,3,4,5,6] -> [1,2,3][4,5,6]\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i: i+batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                                batch_size=16, device=device,\n",
    "                                column_text=\"article\",\n",
    "                                column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total = len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
    "                           padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        '''parameter for length penalty ensures that the model does not generate sequences that are too long.'''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the token, and add the decoded texts with the reference to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"\",\" \") for d in decoded_summaries]\n",
    "\n",
    "        metric.add_batch(predictions = decoded_summaries, references=target_batch)  ## Whatever decoded summaries we have and our target batch will get compared\n",
    "\n",
    "    # Finally compute and return the ROGUE scores.\n",
    "    score = metric.compute()\n",
    "    return score    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "# rogue_metric = load_metric('rogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97284443",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary = 'summary'\n",
    ") ## Calculating for top 10 data itself\n",
    "\n",
    "## Directly use the scores without accessing fmeasure or mid\n",
    "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easy visualization\n",
    "import pandas as pd\n",
    "pd.DataFrame(rouge_dict, index=[f'pegasus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62b766",
   "metadata": {},
   "source": [
    "## Interpreting Good Vs Bad ROUGE Scores:\n",
    "\n",
    "1. Scores close to 1: This indicates a strong overlap between the generated summary and the reference summary, which is desirable in summarization tasks. For example, an F1-score of 0.7 or higher across metrics is generally considered good.\n",
    "2. Scores between 0.5 and 0.7: Indicates moderate overlap. The summary might be capturing key points but is likely missing some structure or important information.\n",
    "3. Scores below 0.5: Suggest a poor match between the generated and reference summaries. The model might be generating irrelevant or incomplete summaries that don't capture the ideas well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c959c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"length_penalty\":0.8, \"num_beams\":8, \"max_length\":128}\n",
    "\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "\n",
    "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\", tokenizer=tokenizer)\n",
    "\n",
    "##\n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)  ## This is my true summary\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"]) ## This is my generated summary from the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
